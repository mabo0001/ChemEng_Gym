{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Env.DC_gym import DiscreteGymDC\n",
    "from Nets.DQN import DQN\n",
    "from Utils.memory import Memory\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PARAM\"\"\"\n",
    "total_episodes = 3000\n",
    "target_updates = 50\n",
    "mem_length = 500\n",
    "gamma = 0.99\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(start_probability, end_probability, current_episode, total_episodes):\n",
    "    epsilon = start_probability + (end_probability-start_probability)*current_episode/total_episodes\n",
    "    random_number = np.random.sample(1)\n",
    "    if epsilon > random_number: # exploit\n",
    "        return True\n",
    "    else: # explore\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# later can turn all of this into a DQN class\n",
    "env = DiscreteGymDC(os.path.join(os.getcwd(), \"Env\\Flowsheet2_PR.fsd\"))\n",
    "DQN_model = DQN(env.n_actions, env.State.state.shape).model\n",
    "target_model = DQN(env.n_actions, env.State.state.shape).model\n",
    "\"\"\"\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DQN_model = DQN(n_actions, env.observation_space.shape, schedule_lr=True).model\n",
    "targetDQN_model = DQN(n_actions, env.observation_space.shape).model\n",
    "memory = Memory(max_size=mem_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(DQN_model, show_shapes=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first populate memory with random experience\n",
    "for i in range(mem_length):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        memory.add([state, action, reward, next_state, 1 - done])\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "epsilon = 0.95\n",
      "10.0\n",
      "epsilon = 0.9349999999999999\n",
      "10.0\n",
      "epsilon = 0.9199999999999999\n",
      "11.0\n",
      "epsilon = 0.9049999999999999\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for i in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    k = 0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        if eps_greedy(0.95, 0.05, i, total_episodes) is True:\n",
    "            action = np.argmax(DQN_model.predict(state[np.newaxis, :]))\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # now take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        memory.add((state, action, reward, next_state, 1 - done))\n",
    "        batch = memory.sample(batch_size)\n",
    "        state_batch = np.array([each[0] for each in batch])\n",
    "        action_batch = np.array([each[1] for each in batch])\n",
    "        reward_batch = np.array([each[2] for each in batch])\n",
    "        next_state_batch = np.array([each[3] for each in batch])\n",
    "        done_batch = np.array([each[4] for each in batch])\n",
    "        \n",
    "        next_action = np.argmax(DQN_model.predict(next_state_batch), axis=1)\n",
    "        y = DQN_model.predict(state_batch) # dummy values for actions that aren't taken\n",
    "        y[np.arange(batch_size), action_batch] = reward + done*gamma*targetDQN_model.predict(next_state_batch)[np.arange(batch_size), next_action]\n",
    "        \n",
    "        DQN_model.train_on_batch(x = state_batch, y=y)\n",
    "        if k > 200: \n",
    "            done = True\n",
    "        \n",
    "        if done is True:\n",
    "            history.append(total_reward)\n",
    "            \n",
    "            \n",
    "    if i % target_updates == 0:\n",
    "        targetDQN_model.set_weights(DQN_model.get_weights())\n",
    "        print(total_reward)\n",
    "        print(f\"epsilon = {0.95 + (0.05-0.95)*i/total_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hist = []\n",
    "for _ in range(10):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = np.argmax(DQN_model.predict(state[np.newaxis, :]))\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    test_hist.append(total_reward)\n",
    "env.close()\n",
    "test_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(DQN_model.predict(next_state_batch), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.argmax(DQN_model.predict(next_state_batch), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_model.predict(next_state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DQN_model.predict(next_state_batch)[np.arange(batch_size), np.argmax(DQN_model.predict(next_state_batch), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = np.array([each[0] for each in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_model.predict(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = DQN_model.predict(state_batch)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_batch = np.array([each[1] for each in batch])\n",
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[np.arange(batch_size), action_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
