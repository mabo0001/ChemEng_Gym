{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from Env.DC_gym import DiscreteGymDC\n",
    "from Nets.DQN import DQN\n",
    "from Utils.memory import Memory\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales prices\n",
    " - these probably need to be done more legitly later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166011.58088, 139084.33956, 227057.35248, 302385.4713 ,\n",
       "       276757.00884, 606271.23792, 606271.23792])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently from thesis code, just copying iso & n butane to be same values\n",
    "compound_names = [\"Methane\", \"Ethane\", \"Propane\", \"Isobutane\", \"Butane\",  \"Iso-Pentane\", \"N-pentane\"]\n",
    "Molar_weights = np.array([16.043, 30.07, 44.097, 58.124, 58.124, 72.151, 72.151])\n",
    "Heating_value = np.array([55.6,  51.9, 50.4, 49.5, 49.4,   55.2, 55.2]) #MJ/kg\n",
    "Price_per_MBTU = np.array([2.83, 2.54, 4.27, 5.79, 5.31,  10.41, 10.41])  #  $/Million Btu\n",
    "\n",
    "sales_prices = Price_per_MBTU*1055.06 * Heating_value  # now in $/kg\n",
    "sales_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChemEng Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_file = \"Env\\ChemSepExample.fsd\"\n",
    "env = DiscreteGymDC(os.path.join(os.getcwd(), COCO_file), sales_prices, n_discretisations=3)\n",
    "state = env.reset()\n",
    "n_actions = env.n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(np.random.choice(env.legal_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PARAM\"\"\"\n",
    "total_episodes = 100\n",
    "target_updates = 50\n",
    "mem_length = 200\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "\n",
    "layer_size = 32\n",
    "depth = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole Env (just to test Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#PARAM\n",
    "total_episodes = 1000\n",
    "target_updates = 200\n",
    "mem_length = 100\n",
    "gamma = 0.95\n",
    "batch_size = 32\n",
    "\n",
    "layer_size = 32\n",
    "depth = 3\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to DQN tester general stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(current_episode, total_episodes, start_probability=0.99, end_probability=0.01):\n",
    "    epsilon = start_probability + (end_probability-start_probability)*current_episode/total_episodes\n",
    "    random_number = np.random.sample(1)\n",
    "    if epsilon > random_number: # exploit\n",
    "        return True\n",
    "    else: # explore\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For testing with cartpole\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DQN_model = DQN(n_actions, env.observation_space.shape, layer_size=layer_size, depth = depth).model\n",
    "#DQN_model = DQN(n_actions, env.observation_space.shape, schedule_lr=True, decay_steps=total_episodes/10).model\n",
    "targetDQN_model = DQN(n_actions, env.observation_space.shape, layer_size=layer_size, depth = depth).model\n",
    "memory = Memory(max_size=mem_length)\n",
    "\"\"\"\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DQN_model = DQN(n_actions, env.State.state.shape, layer_size=layer_size, depth = depth).model\n",
    "targetDQN_model = DQN(n_actions, env.State.state.shape, layer_size=layer_size, depth = depth).model\n",
    "memory = Memory(max_size=mem_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(DQN_model, show_shapes=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when print error function is on, the errors seem to come in pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first populate memory with random experience\n",
    "for i in range(mem_length):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        #action = env.action_space.sample()\n",
    "        action = np.random.choice(env.legal_actions)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        memory.add([state, action, reward, next_state, 1 - done])\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for i in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    k = 0\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        if eps_greedy(i, total_episodes*4/5) is True:\n",
    "            DQN_predictions = DQN_model.predict(state[np.newaxis, :])\n",
    "            mask = np.ones_like(DQN_predictions, bool)\n",
    "            mask[:, env.legal_actions] = False\n",
    "            DQN_predictions[mask] = None\n",
    "            action = np.nanargmax(DQN_predictions)\n",
    "        else:\n",
    "            #action = env.action_space.sample()\n",
    "            action = np.random.choice(env.legal_actions)\n",
    "        \n",
    "        # now take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        memory.add((state, action, reward, next_state, 1 - done))\n",
    "        batch = memory.sample(batch_size)\n",
    "        state_batch = np.array([each[0] for each in batch])\n",
    "        action_batch = np.array([each[1] for each in batch])\n",
    "        reward_batch = np.array([each[2] for each in batch])\n",
    "        next_state_batch = np.array([each[3] for each in batch])\n",
    "        done_batch = np.array([each[4] for each in batch])\n",
    "        \n",
    "        next_action = np.argmax(DQN_model.predict(next_state_batch), axis=1)\n",
    "        y = DQN_model.predict(state_batch) # dummy values for actions that aren't taken\n",
    "        y[np.arange(batch_size), action_batch] = reward_batch + done*gamma*targetDQN_model.predict(next_state_batch)[np.arange(batch_size), next_action]\n",
    "        \n",
    "        DQN_model.train_on_batch(x = state_batch, y=y)\n",
    "  \n",
    "    history.append(total_reward)         \n",
    "            \n",
    "    if i % target_updates == 0:\n",
    "        targetDQN_model.set_weights(DQN_model.get_weights())\n",
    "        print(f\"Average reward of last {target_updates} episodes: {np.mean(history[-target_updates:])}\")\n",
    "        print(f\"episode {i}/{total_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=20):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[N:] - cumsum[:-N]) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(history)\n",
    "plt.plot(running_mean(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiscreteGymDC' object has no attribute 'render'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1b1ae38fe71a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDQN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DiscreteGymDC' object has no attribute 'render'"
     ]
    }
   ],
   "source": [
    "test_hist = []\n",
    "for _ in range(10):\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = np.argmax(DQN_model.predict(state[np.newaxis, :]))\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "    test_hist.append(total_reward)\n",
    "env.close()\n",
    "np.mean(test_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_solves': 5524, 'error_solves': 366}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.error_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
